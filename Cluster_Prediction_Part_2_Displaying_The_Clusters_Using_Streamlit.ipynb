{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mr-8YEJE1Xr3"
      },
      "outputs": [],
      "source": [
        "!pip install -q streamlit\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!npm install localtunnel\n"
      ],
      "metadata": {
        "id": "2fewS2xgBaQ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import silhouette_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "st.title(\"Task 1: Clustering\")\n",
        "\n",
        "train_data = pd.read_excel(\"/content/train.xlsx\")\n",
        "target_variable = train_data.pop(\"target\")\n",
        "\n",
        "#Selecting 5000 random rows to significantly reduce the training time\n",
        "sample_indices = train_data.sample(n=5000, random_state=42).index  # Getting indices for alignment\n",
        "selected_data = train_data.loc[sample_indices]\n",
        "selected_target = target_variable.loc[sample_indices]  # Align with selected_data's index\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(selected_data, selected_target, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "st.write(\"A note on how I approached the problem:  I tried using other algortihms for this problem including K-means, but SVC was showing better silhouette score. So I chose the SVC model and further finetuned the hyperparameters using Grid Search. I have not shown the implementation of either Grid Search or Kmeans in here because it would make the code extremely long and hard to read and also Grid Search takes a huge amount of time to come up with the best parameters.\")\n",
        "\n",
        "# Preprocessing the numerical features\n",
        "scaler = StandardScaler()\n",
        "scaled_train_data = scaler.fit_transform(X_train)\n",
        "\n",
        "# SVC model (replace with best hyperparameters from grid search)\n",
        "svc = SVC(C=0.1, gamma=0.1, kernel='rbf')\n",
        "\n",
        "svc.fit(scaled_train_data, y_train)\n",
        "\n",
        "# Predicting cluster labels\n",
        "train_cluster_labels = svc.predict(scaled_train_data)\n",
        "\n",
        "# Calculating silhouette score (optional)\n",
        "silhouette_score_train = silhouette_score(scaled_train_data, train_cluster_labels)\n",
        "print(f\"Silhouette score (train): {silhouette_score_train:.4f}\")\n",
        "\n",
        "# A new data point (for testing purpose)\n",
        "new_data_point = pd.DataFrame({\"T1\":-70,\"T2\":-59,\"T3\":-67,\"T4\":-58,\"T5\":-91,\"T6\":-99,\"T7\":-76,\"T8\":-54,\"T9\":-93,\"T10\":-72,\"T11\":-83,\"T12\":-54,\"T13\":-77,\"T14\":-65,\"T15\":-82,\"T16\":-88,\"T17\":-54,\"T18\":-76}, index=[0])\n",
        "\n",
        "st.write(\"New data point added after having trained the model\")\n",
        "st.dataframe(new_data_point)\n",
        "\n",
        "# Preprocessing\n",
        "scaled_new_data_point = scaler.transform(new_data_point)\n",
        "\n",
        "# Using the best model to predict the cluster label\n",
        "predicted_cluster = svc.predict(scaled_new_data_point)\n",
        "\n",
        "print(f\"Predicted cluster for the new data point: {predicted_cluster[0]}\")\n",
        "\n",
        "st.write(f\"Predicted cluster for the new data point: {predicted_cluster[0]}\")\n",
        "\n",
        "# Visualizing the cluster with the new data point\n",
        "tsne = TSNE(n_components=2, random_state=42)  # Set random_state for reproducibility\n",
        "reduced_train_data = tsne.fit_transform(scaled_train_data)\n",
        "reduced_new_data_point = tsne.embedding_[0]  # Access the embedded new data point\n",
        "\n",
        "\n",
        "# Converting string labels to numeric values for visualization\n",
        "unique_labels = np.unique(train_cluster_labels)  # Get unique labels\n",
        "label_to_number = {label: i for i, label in enumerate(unique_labels)}  # Create a mapping dictionary\n",
        "numeric_labels = np.array([label_to_number[label] for label in train_cluster_labels])  # Convert labels\n",
        "\n",
        "# Visualization\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.scatter(reduced_train_data[:, 0], reduced_train_data[:, 1], c=numeric_labels, cmap='viridis',)\n",
        "\n",
        "legend_labels = [f\"Cluster {label}\" for label in unique_labels]\n",
        "plt.legend(legend_labels, title=\"Clusters\")\n",
        "plt.scatter(reduced_new_data_point[0], reduced_new_data_point[1], c='red', marker='x', label='New Data Point')\n",
        "plt.title(\"Cluster with New Data Point\")\n",
        "plt.xlabel(\"Dimension 1\")\n",
        "plt.ylabel(\"Dimension 2\")\n",
        "plt.legend(loc='upper right')\n",
        "plt.tight_layout()\n",
        "st.write(\"New data point plotted against previous data points to show the user visually which cluster the new data point belongs to. Each colour here is a different cluster. Since the number of clusters or labels were immense I used TSNE to reduce or scale it down for the purpose of clear visualization.\")\n",
        "st.pyplot(plt.gcf())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UM8JAZ-_pnQM",
        "outputId": "e73e6fd6-dac6-4bbf-d079-e592867ec5ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run /content/app.py &>/content/logs.txt &"
      ],
      "metadata": {
        "id": "GEGU3ESx_0PA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!npx localtunnel --port 8501"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gtkmQPbqACmB",
        "outputId": "1b0400ba-ae0a-48e0-cea4-20f898086bf6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K\u001b[?25hnpx: installed 22 in 1.416s\n",
            "your url is: https://many-frogs-try.loca.lt\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "st.title(\"Task 2: Clasification\")\n",
        "\n",
        "data = pd.read_excel(\"/content/train.xlsx\")\n",
        "X = data.drop(\"target\", axis=1)  # Features\n",
        "y = data[\"target\"]  # Target variable\n",
        "st.write(\"Algorithm Chosen: Random Forest\")\n",
        "# Splitting data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "# training with the Random Forest model\n",
        "model = RandomForestClassifier()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Making predictions on test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluating accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Train accuracy: {accuracy:.4f}\")\n",
        "st.write(f\"Training accuracy: {accuracy:.4f}\")\n",
        "st.write(\"I tried using two Classifiers, SVM and Random Forest. The reason for that was Random Forest offers interpretability and handles potential imbalances well, while SVM excels at finding clear boundaries between classes. I tried both and Random Forest had the highest training accuracy on this dataset, that is why I decided to choose this algorithm.\")\n",
        "\n",
        "test_data = pd.read_excel(\"/content/test.xlsx\")\n",
        "\n",
        "X_test = test_data\n",
        "\n",
        "# Generating predictions with Random Forest\n",
        "predicted_targets = model.predict(X_test)\n",
        "\n",
        "test_data[\"predicted_target\"] = predicted_targets\n",
        "\n",
        "\n",
        "print(test_data[\"predicted_target\"])\n",
        "st.write(\"Predicted target values on the test dataset:\")\n",
        "st.dataframe(test_data[\"predicted_target\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vnAHsO7fPY50",
        "outputId": "0e5ecdcb-5059-4235-b979-f10218660152"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run /content/app.py &>/content/logs.txt &"
      ],
      "metadata": {
        "id": "K7VL_PmXEozf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!npx localtunnel --port 8501"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7FQ-aRquEqtg",
        "outputId": "8e439af4-9a3b-4033-ee67-fcdd4b988807"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K\u001b[?25hnpx: installed 22 in 1.956s\n",
            "your url is: https://hungry-readers-hear.loca.lt\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Exporting the target values to Excel file so that it can be easily shared\n",
        "test_data[\"predicted_target\"].to_excel(\"target_values.xlsx\", index=False)"
      ],
      "metadata": {
        "id": "MZJxKgX8ZpjV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "st.title(\"Task 3(2): Datewise summation of picking and placing activity done.\")\n",
        "\n",
        "df = pd.read_excel(\"/content/rawdata.xlsx\")\n",
        "# Counting picking and placing activities by date\n",
        "picking_counts = df[df['activity'] == 'picked'].groupby('date')['number'].sum()\n",
        "placing_counts = df[df['activity'] == 'placed'].groupby('date')['number'].sum()\n",
        "\n",
        "\n",
        "results = pd.DataFrame({'Picking': picking_counts, 'Placing': placing_counts})\n",
        "print(results)\n",
        "\n",
        "\n",
        "st.header(\"Datewise Picking/Placing Activity\")\n",
        "\n",
        "# Displaying the DataFrame directly with st.dataframe\n",
        "st.dataframe(results)\n",
        "\n",
        "# Displaying individual counts with st.metric\n",
        "st.metric(label=\"Total Picking Activities\", value=results['Picking'].sum())\n",
        "st.metric(label=\"Total Placing Activities\", value=results['Placing'].sum())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vC2a3dC3y-KZ",
        "outputId": "adf6ab51-ea78-4f9a-8419-beb38010f63b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run /content/app.py &>/content/logs.txt &"
      ],
      "metadata": {
        "id": "OBFdobn63y7o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!npx localtunnel --port 8501\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m3jLkrSX3_Ke",
        "outputId": "6a9af541-be65-49e8-8aad-2f27ccdbce94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K\u001b[?25hnpx: installed 22 in 1.606s\n",
            "your url is: https://giant-baths-think.loca.lt\n",
            "^C\n"
          ]
        }
      ]
    }
  ]
}